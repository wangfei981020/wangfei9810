apiVersion: pingcap.com/v1alpha1
kind: TidbCluster
metadata:
  name: g101-prod
  namespace: g101-tidb
spec:
  #version: v8.1.0
  version: v8.5.1
  timezone: UTC
  hostNetwork: true
  configUpdateStrategy: RollingUpdate
  pvReclaimPolicy: Retain
  schedulerName: default-scheduler
  topologySpreadConstraints:
  - topologyKey: kubernetes.io/hostname
  enableDynamicConfiguration: true
  discovery:
    limits:
      cpu: "0.2"
    requests:
      cpu: "0.2"
  helper:
    image: alpine:3.16.0

  pd:
    baseImage: pingcap/pd
    maxFailoverCount: 0
    nodeSelector:
      tier: db
      dbtype: tidb
    tolerations:
    - effect: NoSchedule
      key: tidb-dedicated
      operator: Equal
      value: "true"
    replicas: 3
    requests:
      cpu: "1"
      memory: 2Gi
      storage: 200Gi
    limits:
      cpu: "3"
      memory: 6Gi
      storage: 200Gi
    storageClassName: "gp3"
    config: |
      [dashboard]
        internal-proxy = true
      [replication]
        location-labels = ["kubernetes.io/hostname"]
        max-replicas = 3
      [log]
        level = "error"

  tikv:
    baseImage: pingcap/tikv
    maxFailoverCount: 0
    replicas: 3
    requests:
      cpu: "6"
      memory: 26Gi
      storage: "500Gi"
    limits:
      cpu: "7"
      memory: 28Gi
    config: |
      memory-usage-limit = "26G"
      [storage]
        [storage.block-cache]
          capacity = "16GB"
          shared = true
      [log]
        level = "error"
    storageClassName: "gp3"
    nodeSelector:
      tier: db
      dbtype: tikv 
    tolerations:
    - effect: NoSchedule
      key: tikv-dedicated
      operator: Equal
      value: "true"
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/component
              operator: In
              values:
              - tikv
          topologyKey: kubernetes.io/hostname

  tidb:
    baseImage: pingcap/tidb
    maxFailoverCount: 0
    replicas: 3
    requests:
      cpu: "4"
      memory: 8Gi
      storage: 10Gi
    limits:
      cpu: "5"
      memory: 24Gi
    storageClassName: "gp3"
    service:
      type: ClusterIP
    config: |
      [performance]
        tcp-keep-alive = true
        split-table = true
        oom-action = "cancel"
      [log]
        level = "error"
    annotations:
      tidb.pingcap.com/sysctl-init: "true"
    podSecurityContext:
      sysctls:
      - name: net.ipv4.tcp_keepalive_time
        value: "300"
      - name: net.ipv4.tcp_keepalive_intvl
        value: "75"
      - name: net.core.somaxconn
        value: "32768"
    separateSlowLog: true
    nodeSelector:
      tier: db
      dbtype: tidb
    tolerations:
    - effect: NoSchedule
      key: tidb-dedicated
      operator: Equal
      value: "true"

  #ticdc:
  #  baseImage: pingcap/ticdc
  #  replicas: 1
