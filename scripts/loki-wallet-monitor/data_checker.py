# data_check.py
import os
import re
import time
import pytz
import logging
import requests
from datetime import datetime, timedelta
from urllib.parse import urlparse
from collections import defaultdict
from metrics import (
    JOB_EXECUTION_COUNTER,
    JOB_DURATION_GAUGE,
    LOG_PROCESSED_GAUGE,
    LAST_SUCCESS_GAUGE,
    RESPONSE_STATUS_GAUGE
)

from get_nacos_config import get_config_nacos
from lark_alert import alert_lark, alert_text, alert_text1

logger = logging.getLogger(__name__)

# å¸¸é‡å®šä¹‰
BASE_FILE_NAME = "./logs/data_wallet.txt"
DATE_FORMAT = "%Y-%m-%d"
LINE_PATTERN = re.compile(
    r"^(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d+)\s+([a-f0-9-]+)\s+(https?://\S+)\s+(\d+)$"
)
LOG_PARSE_PATTERN = re.compile(
    r"(?P<time>\d{4}-\d{1,2}-\d{1,2} \d{1,2}:\d{1,2}:\d{1,2}\.\d+)|"
    r"tid:(?P<tid>[a-f0-9-]+)|"
    r"(?P<url>https?://\S+)|running time = (?P<ms>\d+) ms"
)

# ======================
# æ ¸å¿ƒä¸šåŠ¡æ–¹æ³•
# ======================
def _query_loki(loki_url, query, namespace, container):
    """æ‰§è¡ŒLokiæŸ¥è¯¢ï¼ˆä¸¥æ ¼5åˆ†é’Ÿçª—å£ï¼‰"""
    try:
        end_time = int(time.time())
        start_time = end_time - 300  
        response = requests.get(
            loki_url,
            params={
                "query": query,
                "limit": 5000,
                "start": start_time,
                "end": end_time
            },
            timeout=15
        )
        response.raise_for_status()
        return response.json()["data"]["result"]
    except Exception as e:
        logger.error(f"LokiæŸ¥è¯¢å¤±è´¥: {str(e)} [å®¹å™¨: {container}]")
        return None

def _send_alert(config, record):
    config = get_config_nacos()
    """å‘é€å®æ—¶æŠ¥è­¦"""
    try:
        content = alert_text1(
            record["tid"], 
            record["url"], 
            record["duration"], 
            record["timestamp"]
        )
        mentioned_list = []
        for j in config["lark_id"].values():
            mentioned = f"<at id={j}>@{j}</at"
            mentioned_list.append(mentioned)
        mentioned_str = ",".join(mentioned_list)
        # content.append(mentioned_str) 
        alert_lark(
            config["webhook_url"],
            "ğŸš¨ å‘Šè­¦é€šçŸ¥",
            "red",
            content,
            mentioned_str
        )
    except Exception as e:
        logger.error(f"å‘é€æŠ¥è­¦å¤±è´¥: {str(e)}")

def process_and_report(file_path, config):
    """å¤„ç†æ—¥å¿—å¹¶å‘é€æŠ¥å‘Š"""
    domain_stats = defaultdict(lambda: {
        "count": 0, 
        "total": 0, 
        "min": float("inf"), 
        "max": 0
    })
    
    try:
        with open(file_path, "r") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                
                match = LINE_PATTERN.match(line)
                if not match:
                    logger.warning(f"æ— æ•ˆæ—¥å¿—æ ¼å¼: {line}")
                    continue
                
                timestamp,tid, url, duration = match.groups()
                try:
                    domain = urlparse(url).netloc
                    duration = int(duration)
                except Exception as e:
                    logger.error(f"æ•°æ®è§£æå¤±è´¥: {str(e)} [æ•°æ®: {line}]")
                    continue
                
                stats = domain_stats[domain]
                stats["count"] += 1
                stats["total"] += duration
                stats["min"] = min(stats["min"], duration)
                stats["max"] = max(stats["max"], duration)

        # å‘é€ç»Ÿè®¡æŠ¥å‘Š
        for domain, stats in domain_stats.items():
            avg = stats["total"] / stats["count"] if stats["count"] else 0
            report_time = datetime.now(pytz.timezone("Asia/Shanghai")).strftime("%Y-%m-%d %H:%M:%S")
            content = alert_text(
                domain,
                stats["count"],
                stats["min"],
                f"{avg:.2f}",
                stats["max"],
                report_time
            )
            mentioned = [f"<at id={id}>@{id}</at>" for id in config["lark_id"].values()]
            alert_lark(
                config["webhook_url"],
                "ğŸ“Š æ¯æ—¥æ€§èƒ½æŠ¥å‘Š",
                "green",
                content,
                ",".join(mentioned)
            )
            
    except FileNotFoundError:
        logger.error(f"æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    except Exception as e:
        logger.error(f"æ—¥å¿—å¤„ç†å¼‚å¸¸: {str(e)}")

def _cleanup_old_logs(ref_date):
    """æ¸…ç†7å¤©å‰çš„æ—¥å¿—æ–‡ä»¶"""
    try:
        cutoff = ref_date - timedelta(days=7)
        pattern = re.compile(rf"^{re.escape(BASE_FILE_NAME)}-(\d{{4}}-\d{{2}}-\d{{2}})$")
        
        for filename in os.listdir("."):
            match = pattern.match(filename)
            if not match:
                continue
            
            try:
                file_date = datetime.strptime(match.group(1), DATE_FORMAT).date()
                if file_date < cutoff.date():
                    os.remove(filename)
                    logger.info(f"å·²æ¸…ç†è¿‡æœŸæ—¥å¿—: {filename}")
            except ValueError:
                logger.warning(f"æ— æ•ˆæ–‡ä»¶åæ ¼å¼: {filename}")
            except Exception as e:
                logger.error(f"åˆ é™¤æ–‡ä»¶å¤±è´¥ {filename}: {str(e)}")
                
    except Exception as e:
        logger.error(f"æ—¥å¿—æ¸…ç†å¤±è´¥: {str(e)}")

# ======================
# å®šæ—¶ä»»åŠ¡å…¥å£
# ======================
def background_job():
    """å®šæ—¶æ•°æ®æ”¶é›†ä»»åŠ¡ï¼ˆä¸¥æ ¼å¯¹é½5åˆ†é’Ÿï¼‰"""
    job_name = "5min_collect_job"
    start_time = time.time()
    
    try:
        logger.info("ğŸš€ å¯åŠ¨æ•°æ®æ”¶é›†ä»»åŠ¡...")
        config = get_config_nacos()
        results = []
        
        for namespace_group in config["containers"]:
            namespace = namespace_group["namespace"]
            for container in namespace_group["containers"]:
                query = f'{{namespace="{namespace}",container="{container}"}} |~ "è°ƒç”¨ç«™ç‚¹äº¤æ˜“æ¥å£"|="http"|="æ€§èƒ½"'
                data = _query_loki(config["loki_url"], query, namespace, container)
                
                if data:
                    for entry in data:
                        for _, log in entry.get("values", []):
                            matches = LOG_PARSE_PATTERN.findall(log)
                            record = {"timestamp": "", "url": "", "duration": ""}
                            for time_part, tid_part,url_part, ms_part in matches:
                                if time_part: record["timestamp"] = time_part
                                if tid_part: record["tid"] = tid_part
                                if url_part: record["url"] = url_part
                                if ms_part: record["duration"] = ms_part
                            
                            if all(record.values()):
                                results.append(record)
                                
                                if int(record["duration"]) > 3000:
                                    _send_alert(config, record)

        if results:
            domain_max_list = []
            domain_min_list = []
            with open(BASE_FILE_NAME, "a") as f:
                for item in results:
                    f.write(f"{item['timestamp']} {item['tid']} {item['url']} {item['duration']}\n")
                    duration = int(item["duration"])
                    if duration > 1000:
                        domain_url = item['url']
                        if domain_url not in domain_max_list:
                            domain_max_list.append(domain_url)

            if domain_max_list:
                for m in list(set(domain_max_list)):
                    status = 1 
                    RESPONSE_STATUS_GAUGE.labels(url=m).set(status)
                    if m in domain_min_list:
                        domain_min_list.remove(m)
                        logger.info(f"{m} å·²åˆ é™¤ï¼Œå½“å‰åˆ—è¡¨:", domain_min_list)
  
            if domain_min_list:
                for d in list(set(domain_min_list)):
                    status = 0
                    RESPONSE_STATUS_GAUGE.labels(url=d).set(status)
            logger.info(f"ğŸ“¥ æˆåŠŸå†™å…¥ {len(results)} æ¡æ—¥å¿—è®°å½•")
            
            
        LOG_PROCESSED_GAUGE.labels(job_name).set(len(results))
        LAST_SUCCESS_GAUGE.labels(job_name).set_to_current_time()
        JOB_EXECUTION_COUNTER.labels(job_name, 'success').inc()
        
    except Exception as e:
        logger.error(f"æ•°æ®æ”¶é›†ä»»åŠ¡å¼‚å¸¸: {str(e)}")
        JOB_EXECUTION_COUNTER.labels(job_name, 'failed').inc()
    finally:
        JOB_DURATION_GAUGE.labels(job_name).set(time.time() - start_time)

def daily_job():
    """æ¯æ—¥æ—¥å¿—å¤„ç†ä»»åŠ¡"""
    job_name = "daily_report_job"
    start_time = time.time()
    
    try:
        logger.info("ğŸŒ™ å¯åŠ¨æ¯æ—¥æ—¥å¿—å¤„ç†...")
        config = get_config_nacos()
        tz = pytz.timezone("Asia/Shanghai")
        now = datetime.now(tz)
        
        if os.path.exists(BASE_FILE_NAME):
            process_and_report(BASE_FILE_NAME, config)
            new_name = f"{BASE_FILE_NAME}-{now.strftime(DATE_FORMAT)}"
            os.rename(BASE_FILE_NAME, new_name)
            logger.info(f"ğŸ”„ æ–‡ä»¶å·²é‡å‘½åä¸º: {new_name}")
            open(BASE_FILE_NAME, "a").close()
        else:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°å½“æ—¥æ—¥å¿—æ–‡ä»¶")
            
        _cleanup_old_logs(now)
        
        LAST_SUCCESS_GAUGE.labels(job_name).set_to_current_time()
        JOB_EXECUTION_COUNTER.labels(job_name, 'success').inc()
        
    except Exception as e:
        logger.error(f"æ¯æ—¥ä»»åŠ¡å¤±è´¥: {str(e)}")
        JOB_EXECUTION_COUNTER.labels(job_name, 'failed').inc()
    finally:
        JOB_DURATION_GAUGE.labels(job_name).set(time.time() - start_time)